There are product attributes for some products. there are product descriptions for each product and can be merged on the produt_uid.

perhaps a model that is ONLY trained off the product title

another model that is ONLY using the description

another model that uses BOTH PT and PD

and then another model that uses the product attributes 
	BECAUSE it is NOT a complete file (1-to-1), it can only train for "what's there"

there is also a combined fourth model that uses them all

then there are flavors of models:
random forest
logistic regression
SVM
ANN

TF/IDF and cosine matrix for them all. the "distance" metric seems like it would be skewed if there are missing values though.

---- PROCESS

TODO  * run spell check on the data and save a "clean file"
√  * run a stop-words on the data and save a "clean file"
√  * conduct a root-word analysis 

TODO 
√ * make sure all text is in lower case (for standardization)
√ * run spell check on the data and save a "clean file"
√* keyword count  -> currently as an aggregate. could maybe do them separately
	- title
	- description
	- attributes
	- aggregate 
* round and separate classifications
	1.1 -> 1  :: 1.5 -> 2 :: 2.8 -> 3, etc
	-- setup for three classification run w/ predict_proba
	-- argmax() to select highest probability and set classificiation
		-> COULD setup a voting with the args though...or an avg of the rating... 
* thesaurs on search query  
* translate ?? 
	--> at least one instance of spanish: aspiradora (vacuum)


--- V1  :: POC 
predictions_v1.csv
- just a logit using a single feature (cosine from product title and search query)
 :: results - bad. haha 173x of 183x haha


_v1-2.csv
- just an SVM of the same

	:: v2 	::
	:: v2_a ::
- will include a new cosine calculation for the spell-checked search queries
- next up will be to do keyword counts, likely in a similar fashion as to what was done w/ crowdsource: kw_1_cnt, kw_2_cnt, etc 
	a_ you might want to consider combining the title & desc so it's only one search through... the search space is pretty large...see when file is written when youg et up in the morning
	-> issues with kw_ratio...not srue what
	-> don't forget you can mess w/ TfidfVectorizer(min_df). it MAY make sense to increase that... 


Forward-THINKING stuff
 * looking at the distribution of attributes, we're about 99% across the board (see below)
 * based on this, i'm inclined to THROW OUT the training sets that DON'T have an attribute and train off that
	-- again using the cosine distance 
	-- since i'll be in there, just going to make the cosine distance off the product description as well
	-- three more features could be
		1) cosine distance from a combination of pt + pd
		2) C-D from pt + attri
		3) C-D from pt + pd + attri
		:: combining the attribute with the pt or pd gives the advantage of having a complete data set
	-- LAST, i'm going to get a "keyword count" for:
		a) keyword match: pt
		b) keyword match: pd
		c) keyword match: attri
	-- UBER LAST: get the damn spell check going! 

 * review https://github.com/ChenglongChen/Kaggle_CrowdFlower
	note: "  2. cosine similarity between query & title, query & description, title & description pairs" @ https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Feat/genFeat_basic_tfidf_feat.py


TRAIN STUFF
Overall completeness of attributes:  0.999215
Relevance == 1 completeness of attributes:  0.999219
Relevance == 2 completeness of attributes:  0.999039
Relevance == 3 completeness of attributes:  0.999257
TRAIN STUFF
Overall completeness of attributes:  0.999164


 ---- **  V2 ** ----
* got spell check working (but it is sort of weaksauce)
	-- MAYBE need to build a better dictionary :: something hardware specific ??? 
* has C.-D. for search_term -> PT and -> PD
	-- takes a REALLY long time to run for PD...

train / test files:  train_bayes3b_2b-1.csv
	:: 3rd iteration of bayes w/ a) addition of prod_des to text file; b) inclusion of brands in stopwords (not sure it works properly); c) use of enchant.checker to trigger the spelling error
	:: includes kw_matches (as strict count, no ratios)
	:: includes a "raw_cosine" distance for pt and pd and then a spellchecked version ((because there are inconsistencies in output))
		--> thinking is (if time permits) to pursue thesaurus route...saw a top team mentioning it...so probably on to something)

 -- for models, included scaler because the cosine distances are all .xxx and the kw_matches are all ints, so better to scale everything

logit:  v2-1.csv
	- SADLY, this outputs the EXACT SAME THING! WTF??? 
		-> error? 
		-> needs the separation of class??? 

SVM: v2-1b.csv 
	- not sure what happened but output was HIGHER than expected:
		:: got values like 3.4x ...wth? 
		:: MIGHT because i used SVR() (to minimize error)
		:: going to switch to SVM() and treat them all as classes

- going to evaluate stratification in sampling
   values < 2 (1.1, 1.3, 1.x, etc)  (of 74067 in X_train)
	FALSE  TRUE 
	62158 11909  :: 16.08%
   values < 2.5 > 2
	FALSE  TRUE 
	57996 16071  :: 21.70%
   values > 2.5 < 3
	FALSE  TRUE 
	58854 15213 :: 20.53%
  values == 3
	FALSE  TRUE 
	54942 19125 :: 25.82%

-> SO, objective would be to have about 25% across all four "class strata" (<2, >2<2.5, etc)
	process: 
	a) create a df with all <2 (11909)
	b) create a df with all >2 < 2.5
	c) create a df with all >2.5 < 3
	d) create a df with all == 3
	e) take random samples from each df where #_samples == num_in<2_group (11909)

 summary(X_train$relevance)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.000   2.000   2.330   2.382   3.000   3.000 

--> V2B-1 <---
v2b-1.csv (logit)
	:: ran because it's a LOT faster than the SVM
	:: may not submit (to save submissions for day)
	:; looking at output, the predictions now are ONLY 2.33 or 1.67
	:: THINKING about this, it makes sense. i think i need to recode the relevance
		--> enter "rounding" ...heh. 
		keeping groups: pseudo: self.search_inputs.y_train.apply(lambda x: (round(x * 2.0) / 2.0)
vlinear1.csv
	:: this runs the linear regression, so treating them as continuous values
	...seems pretty biased toward mid-to-high "1s" and not toward 2.xx
	:: submitted for benchmark: Your submission scored 0.75622

DIGGING in a bit more, looks like it MAY be better to reclass things a bit more
   values < 1.5  (of 74067 in X_train)
	FALSE  TRUE 
	68952  5115  :: 6.91%
   values <= 2  & 1.5
	FALSE  TRUE 
	55548 18519 :: 
   values < 2.5 > 2
	FALSE  TRUE 
	57996 16071  :: 21.70%
   values > 2.5 < 3
	FALSE  TRUE 
	58854 15213 :: 20.53%
  values == 3
	FALSE  TRUE 
	54942 19125 :: 25.82%

EX: 2b-4.csv
	- uses svr(); again predictions > 3 and < 1
	- SO, putting in ceiling and floors. just need to get a benchmark for performance
EX: 2b-5.csv
	- svr with ceilings and floors



---  ***   V3 *** ---
output of bayes4 completed but svm for v2 still running in gridsearch
	problematic but moving forward w/ experiment
* v3 uses bayes4. initial glance at output shows that cosine distances (in some places) are greatly improved. 
* v3 will also see a separation of kw_matches to: overall, title, description 
	* but still only for search_term_fixes (i.e. not raw)
* bayes4 shows there are still some words that are problematic: lawnmower -> lawn mower, etc
* v3 will be setup to run logit, svm and ann w/ an ensemble
	- file will save one for each model and a final ensemble




Evaluation

Submissions are evaluated on the root mean squared error (RMSE).


Submission File

For each id in the test set, you must predict a relevance. This is a real number in [1,3]. The file should contain a header and have the following format:
id,relevance
1,1
4,2
5,3

