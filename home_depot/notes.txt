There are product attributes for some products. there are product descriptions for each product and can be merged on the produt_uid.

perhaps a model that is ONLY trained off the product title

another model that is ONLY using the description

another model that uses BOTH PT and PD

and then another model that uses the product attributes 
	BECAUSE it is NOT a complete file (1-to-1), it can only train for "what's there"

there is also a combined fourth model that uses them all

then there are flavors of models:
random forest
logistic regression
SVM
ANN

TF/IDF and cosine matrix for them all. the "distance" metric seems like it would be skewed if there are missing values though.

---- PROCESS

TODO  * run spell check on the data and save a "clean file"
√  * run a stop-words on the data and save a "clean file"
√  * conduct a root-word analysis 

TODO 
* make sure all text is in lower case (for standardization)
* run spell check on the data and save a "clean file"
* keyword count
	- title
	- description
	- attributes
	- aggregate 
* round and separate classifications
	1.1 -> 1  :: 1.5 -> 2 :: 2.8 -> 3, etc
	-- setup for three classification run w/ predict_proba
	-- argmax() to select highest probability and set classificiation
		-> COULD setup a voting with the args though...or an avg of the rating... 
* thesaurs on search query
* translate ?? 
	--> at least one instance of spanish: aspiradora (vacuum)


--- V1  :: POC 
predictions_v1.csv
- just a logit using a single feature (cosine from product title and search query)
 :: results - bad. haha 173x of 183x haha


_v1-2.csv
- just an SVM of the same



---- v2 ::
 * looking at the distribution of attributes, we're about 99% across the board (see below)
 * based on this, i'm inclined to THROW OUT the training sets that DON'T have an attribute and train off that
	-- again using the cosine distance 
	-- since i'll be in there, just going to make the cosine distance off the product description as well
	-- three more features could be
		1) cosine distance from a combination of pt + pd
		2) C-D from pt + attri
		3) C-D from pt + pd + attri
		:: combining the attribute with the pt or pd gives the advantage of having a complete data set
	-- LAST, i'm going to get a "keyword count" for:
		a) keyword match: pt
		b) keyword match: pd
		c) keyword match: attri
	-- UBER LAST: get the damn spell check going! 


TRAIN STUFF
Overall completeness of attributes:  0.999215
Relevance == 1 completeness of attributes:  0.999219
Relevance == 2 completeness of attributes:  0.999039
Relevance == 3 completeness of attributes:  0.999257
TRAIN STUFF
Overall completeness of attributes:  0.999164







Evaluation

Submissions are evaluated on the root mean squared error (RMSE).


Submission File

For each id in the test set, you must predict a relevance. This is a real number in [1,3]. The file should contain a header and have the following format:
id,relevance
1,1
4,2
5,3

